{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhCqCRx+m5ReDS5KUQGZjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhinavKumar0000/Data_collection_pipeline/blob/main/Job_Postings_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Objective\n",
        "The objective of this notebook is to document the end-to-end process of constructing a high quality, annotated dataset from raw software engineering job postings. The resulting dataset is intended for use in downstream machine learning applications, such as role classification and skill based candidate matching.\n",
        "\n",
        "## Pipeline Stages\n",
        "The documented pipeline consists of four primary stages:\n",
        "\n",
        "\n",
        "\n",
        "*   **Data Collection**: Sourcing and ingesting raw job data via an API.\n",
        "*   **Data Cleaning and Preprocessing**: Applying standardization and normalization procedures to the text data.\n",
        "*   **Exploratory Data Analysis (EDA)**: Performing quantitative analysis of the corpus to inform the annotation strategy.\n",
        "*   **Data Annotation**: Programmatically applying a data driven labeling schema to create the final, enriched dataset."
      ],
      "metadata": {
        "id": "-nMkK02aHFgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imporing necessary libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tfEsviaYuOnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingests job posting data from the Remotive REST API"
      ],
      "metadata": {
        "id": "TAsWo-evKyQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34QywcE9uFcA"
      },
      "outputs": [],
      "source": [
        "def collect_jobs_from_remotive_api():\n",
        "\n",
        "    #API endpoint and parameters\n",
        "    api_url = \"https://remotive.com/api/remote-jobs?category=software-development&limit=50\"\n",
        "\n",
        "    print(f\"Requesting data from API endpoint: {api_url}\")\n",
        "    job_list = []\n",
        "\n",
        "    try:\n",
        "        response = requests.get(api_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if 'jobs' not in data or not data['jobs']:\n",
        "            print(\"The API response did not contain a 'jobs' array or it was empty\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(f\"Successfully fetched {len(data['jobs'])} records from the API\")\n",
        "\n",
        "        #HTML is parsed to extract plain text\n",
        "        for job in data['jobs']:\n",
        "            description_html = job.get('description', '')\n",
        "            soup = BeautifulSoup(description_html, 'html.parser')\n",
        "            clean_description = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "            #Maping the API response fields to our target schema\n",
        "            job_data = {\n",
        "                'job_title': job.get('title'),\n",
        "                'company_name': job.get('company_name'),\n",
        "                'location': job.get('candidate_required_location'),\n",
        "                'job_description': clean_description,\n",
        "                'url': job.get('url')\n",
        "            }\n",
        "            job_list.append(job_data)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred during the API request: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    return pd.DataFrame(job_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Executing the Data Collection Script"
      ],
      "metadata": {
        "id": "zWWPiS3DLVh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = collect_jobs_from_remotive_api()\n",
        "\n",
        "if not raw_df.empty:\n",
        "    raw_df.to_csv('raw_data.csv', index=False)\n",
        "    print(\"Data collection successful. Raw dataset saved to raw_data.csv\")\n",
        "else:\n",
        "    print(\"Data collection failed or returned no data\")"
      ],
      "metadata": {
        "id": "tlTqfTsEuySd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not raw_df.empty:\n",
        "    print(\"Raw Data:\")\n",
        "    display(raw_df.head())"
      ],
      "metadata": {
        "id": "lt266qW3ziuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.info()"
      ],
      "metadata": {
        "id": "1AGaSR-ixPlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Standardizing Dataset"
      ],
      "metadata": {
        "id": "e4xEGZCjMX6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_job_description(text):\n",
        "\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    #Standardize text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    #Remove any non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    #Collapse multiple whitespace characters into a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "g4MP9s7bw_7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df = pd.read_csv('raw_data.csv')\n",
        "    print(f\"Loaded raw_data.csv with {len(df)} initial records\")\n",
        "\n",
        "    #De-duplicate records based on the unique job URL\n",
        "    df.drop_duplicates(subset=['url'], inplace=True)\n",
        "\n",
        "    #Remove records with null values in the description\n",
        "    df.dropna(subset=['job_description'], inplace=True)\n",
        "    print(f\"After handling duplicates and nulls, {len(df)} records remain\")\n",
        "\n",
        "    #Apply the text cleaning to the job_description column\n",
        "    df['cleaned_description'] = df['job_description'].apply(clean_job_description)\n",
        "\n",
        "    #Select and reorder columns for the cleaned dataset\n",
        "    cleaned_df = df[['job_title', 'company_name', 'location', 'cleaned_description', 'url']]\n",
        "    cleaned_df.to_csv('cleaned_data.csv', index=False)\n",
        "    print(\"Data cleaning complete and saved to cleaned_data.csv\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: raw_data.csv not found\")"
      ],
      "metadata": {
        "id": "2BZBLSo2xH-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'cleaned_df' in locals():\n",
        "    print(\"Cleaned Data:\")\n",
        "    display(cleaned_df.head())"
      ],
      "metadata": {
        "id": "V3VJIiySxcMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.info()"
      ],
      "metadata": {
        "id": "UcT5eqadIIdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis (EDA)**:\n",
        "Before defining an annotation schema, an Exploratory Data Analysis is performed to understand the underlying structure and content of the text corpus. This data-driven approach ensures that the chosen labels are relevant, representative of the data, and will provide maximum value for model training. The analysis focuses on identifying the most frequent terms and phrases."
      ],
      "metadata": {
        "id": "9MHE0GpcIILD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df_eda = pd.read_csv('cleaned_data.csv').dropna(subset=['cleaned_description'])\n",
        "    print(f\"Loaded cleaned_data.csv for analysis, containing {len(df_eda)} records\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: cleaned_data.csv not found\")"
      ],
      "metadata": {
        "id": "oJmfAjzYxi0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Unigram (Single Word) Frequency Analysis\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tvq73n8XUbhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec_unigram = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "unigram_counts = vec_unigram.fit_transform(df_eda['cleaned_description'])\n",
        "sum_words = unigram_counts.sum(axis=0)\n",
        "words_freq = sorted([(word, sum_words[0, idx]) for word, idx in vec_unigram.vocabulary_.items()], key=lambda x: x[1], reverse=True)\n",
        "top_20_words = pd.DataFrame(words_freq[:20], columns=['Word', 'Frequency'])"
      ],
      "metadata": {
        "id": "dLENF-5Uxrfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigram (Two Word Phrase) Frequency Analysis"
      ],
      "metadata": {
        "id": "gI1mB0GOUxvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec_bigram = CountVectorizer(stop_words='english', ngram_range=(2, 2))\n",
        "bigram_counts = vec_bigram.fit_transform(df_eda['cleaned_description'])\n",
        "sum_phrases = bigram_counts.sum(axis=0)\n",
        "phrases_freq = sorted([(phrase, sum_phrases[0, idx]) for phrase, idx in vec_bigram.vocabulary_.items()], key=lambda x: x[1], reverse=True)\n",
        "top_20_phrases = pd.DataFrame(phrases_freq[:20], columns=['Phrase', 'Frequency'])"
      ],
      "metadata": {
        "id": "SZg2XNbkUuhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of EDA Results"
      ],
      "metadata": {
        "id": "Y6oeHTWsaJJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('default')\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
        "fig.suptitle('Job Description Text Analysis', fontsize=20, fontweight='bold', y=0.98)\n",
        "\n",
        "#Plot 1: Top 20 words with horizontal bars\n",
        "bars1 = axes[0].barh(top_20_words['Word'], top_20_words['Frequency'],\n",
        "                    color=plt.cm.Blues(np.linspace(0.4, 0.8, len(top_20_words))),\n",
        "                    edgecolor='black', linewidth=0.5)\n",
        "axes[0].set_title('Top 20 Most Frequent Terms', fontsize=16, fontweight='bold', pad=20)\n",
        "axes[0].set_xlabel('Frequency', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Unigrams', fontsize=12, fontweight='bold')\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "for i, (value, word) in enumerate(zip(top_20_words['Frequency'], top_20_words['Word'])):\n",
        "    axes[0].text(value + max(top_20_words['Frequency'])*0.01, i,\n",
        "                f'{value:,}', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "#Plot 2: Top 20 phrases with different color scheme\n",
        "bars2 = axes[1].barh(top_20_phrases['Phrase'], top_20_phrases['Frequency'],\n",
        "                    color=plt.cm.Oranges(np.linspace(0.4, 0.8, len(top_20_phrases))),\n",
        "                    edgecolor='black', linewidth=0.5)\n",
        "axes[1].set_title('Top 20 Most Frequent Bigrams', fontsize=16, fontweight='bold', pad=20)\n",
        "axes[1].set_xlabel('Frequency', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Bigrams', fontsize=12, fontweight='bold')\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "for i, (value, phrase) in enumerate(zip(top_20_phrases['Frequency'], top_20_phrases['Phrase'])):\n",
        "    axes[1].text(value + max(top_20_phrases['Frequency'])*0.01, i,\n",
        "                f'{value:,}', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ruW4rq8BZoUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Generating a skill list by cross referencing EDA results with a Master list of known Technical Skills"
      ],
      "metadata": {
        "id": "zdwCchytXSMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MASTER_SKILL_LIST = [\n",
        "    #Programming Languages\n",
        "    'python', 'javascript', 'typescript', 'java', 'go', 'golang', 'ruby', 'php', 'c++', 'c#', 'rust', 'kotlin', 'swift',\n",
        "\n",
        "    #Frontend Frameworks & Libraries\n",
        "    'react', 'angular', 'vue', 'svelte', 'next js', 'react native',\n",
        "\n",
        "    #Backend Frameworks & Libraries\n",
        "    'node js', 'django', 'flask', 'fastapi', 'spring', 'ruby on rails',\n",
        "\n",
        "    #Cloud Platforms & Services\n",
        "    'aws', 'gcp', 'azure', 'amazon web services', 'google cloud', 'heroku', 'digitalocean',\n",
        "\n",
        "    #Databases & Caching\n",
        "    'sql', 'nosql', 'postgresql', 'mysql', 'mongodb', 'redis', 'cassandra', 'dynamodb', 'elasticsearch',\n",
        "\n",
        "    #DevOps, Infrastructure & Tooling\n",
        "    'docker', 'kubernetes', 'terraform', 'ansible', 'jenkins', 'ci cd', 'git', 'github', 'gitlab',\n",
        "\n",
        "    #Data Science & ML\n",
        "    'pandas', 'numpy', 'tensorflow', 'pytorch', 'scikit learn', 'apache spark', 'hadoop'\n",
        "]\n",
        "\n",
        "def generate_skills_from_eda(words_freq, phrases_freq, master_list):\n",
        "    frequent_terms = {item[0] for item in words_freq + phrases_freq}\n",
        "    identified_skills = [skill for skill in master_list if skill in frequent_terms]\n",
        "\n",
        "    return identified_skills\n",
        "\n",
        "if 'words_freq' in locals() and 'phrases_freq' in locals():\n",
        "\n",
        "    SKILL_KEYWORDS = generate_skills_from_eda(words_freq, phrases_freq, MASTER_SKILL_LIST)\n",
        "    print(f\"Found {len(SKILL_KEYWORDS)} relevant skills in the dataset based on the EDA\")\n",
        "    print(SKILL_KEYWORDS)\n",
        "else:\n",
        "    print(\"Error: EDA results ('words_freq', 'phrases_freq') not found. Please run the EDA step first\")\n",
        "    #As a fallback, use the full master list\n",
        "    SKILL_KEYWORDS = MASTER_SKILL_LIST"
      ],
      "metadata": {
        "id": "-kUZhBcrWRFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assigning label for experience_level, job_type and technical_skills"
      ],
      "metadata": {
        "id": "gNgHn_xaXWFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_experience_level(desc):\n",
        "\n",
        "    if any(k in desc for k in ['principal', 'lead', 'staff', '10+', '8+ years']): return 'Lead/Principal'\n",
        "    if any(k in desc for k in ['senior', 'sr', '5+ years', '6+ years']): return 'Senior'\n",
        "    if any(k in desc for k in ['mid level', '2+ years', '3+ years']): return 'Mid-Level'\n",
        "    if any(k in desc for k in ['junior', 'jr', 'entry level', 'graduate']): return 'Entry-Level'\n",
        "    return 'Not Specified'\n",
        "\n",
        "def get_job_type(desc):\n",
        "\n",
        "    if any(k in desc for k in ['full stack', 'fullstack']): return 'Full-Stack'\n",
        "    if any(k in desc for k in ['backend', 'back end', 'api']): return 'Backend'\n",
        "    if any(k in desc for k in ['frontend', 'front end', 'ui', 'ux']): return 'Frontend'\n",
        "    if any(k in desc for k in ['mobile', 'ios', 'android']): return 'Mobile'\n",
        "    if any(k in desc for k in ['devops', 'sre', 'infrastructure']): return 'DevOps'\n",
        "    if 'data engineer' in desc: return 'Data'\n",
        "    return 'Not Specified'\n",
        "\n",
        "def extract_skill_tags(desc):\n",
        "\n",
        "    found = [skill for skill in SKILL_KEYWORDS if skill in desc]\n",
        "    return ', '.join(found) if found else 'No Tags Found'"
      ],
      "metadata": {
        "id": "eSAikG8_ybIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df_annotate = pd.read_csv('cleaned_data.csv')\n",
        "    print(f\"Applying annotation rules to {len(df_annotate)} cleaned records\")\n",
        "\n",
        "    df_annotate['experience_level'] = df_annotate['cleaned_description'].apply(get_experience_level)\n",
        "    df_annotate['job_type'] = df_annotate['cleaned_description'].apply(get_job_type)\n",
        "    df_annotate['skill_tags'] = df_annotate['cleaned_description'].apply(extract_skill_tags)\n",
        "\n",
        "    final_columns = ['job_title', 'company_name', 'experience_level', 'job_type', 'skill_tags', 'cleaned_description']\n",
        "    annotated_df = df_annotate[final_columns].head(20)\n",
        "    print(\"Created a sample of 20 annotated records\")\n",
        "\n",
        "    #Saving to CSV format\n",
        "    annotated_df.to_csv('annotated_data.csv', index=False)\n",
        "    print(\"Successfully saved annotated sample to annotated_data.csv\")\n",
        "\n",
        "    #Saving to JSON format\n",
        "    annotated_df.to_json('annotated_data.json', orient='records', indent=4)\n",
        "    print(\"Successfully saved annotated sample to annotated_data.json\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: cleaned_data.csv not found.\")"
      ],
      "metadata": {
        "id": "ZIfVFn110f9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'annotated_df' in locals():\n",
        "    print(\"Annotaed data:\")\n",
        "    display(annotated_df)"
      ],
      "metadata": {
        "id": "u925q1W30lxU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}